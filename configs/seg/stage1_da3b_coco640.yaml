# Optional checkpoint path for resuming training
# ckpt_path: /cache/model/stage1_da3b_coco640/last.ckpt
# load_weights_only: true
# resume_from_weights: /cache/model/stage1_da3b_coco640/last.ckpt

trainer:
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: da3_seg_coco
      name: stage1_da3b_coco640
  devices: 8
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  precision: 16-mixed
  check_val_every_n_epoch: 1
  limit_val_batches: 1.0
  max_epochs: 8
  # ğŸ”§ æ·»åŠ callbacksé…ç½®
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: checkpoints/stage1_da3b_coco640  # ä¿å­˜ç›®å½•
        filename: 'epoch={epoch}-step={step}-loss={train_loss:.4f}'  # æ–‡ä»¶åæ ¼å¼
        monitor: train_loss  # ç›‘æ§çš„æŒ‡æ ‡
        mode: min  # æœ€å°åŒ–loss
        save_top_k: 1  # ä¿å­˜æœ€å¥½çš„3ä¸ª
        save_last: true  # åŒæ—¶ä¿å­˜last.ckpt
        save_on_train_epoch_end: false  # æ¯ä¸ªepochç»“æŸä¿å­˜
        verbose: true
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.ModelSummary
      init_args:
        max_depth: 2

model:
  da3_pretrained_path: "/cache/model/da3/"
  network_config:
    enable_dino_teacher: false
    dino_teacher_ckpt: "/cache/model/dinov3/dinov3_vitb16_pretrain.pth"
    gram_layers: [5, 6, 7, 8, 9, 10, 11]
    lambda_gram: 0.1
    dino_d_base: 384
    net:
      name: vitb
      out_layers: [5, 7, 9, 11]
      alt_start: 4
      qknorm_start: 4
      rope_start: 4
      cat_token: true
      seg_cfg:
        enable: true
        num_bottleneck_tokens: 32
        num_queries: 100
        num_masked_layers: 4
        layer_start: 6
        dropout: 0.0
        attn_dropout: 0.0
        enable_dino_teacher: false
        distill_layers: [5, 6, 7, 8, 9, 10, 11]
        dino_d_base: 384
        dino_teacher_dim: 768
        dino_seg_dim: 768
        enable_seg_adapter: true
    head:
      dim_in: 1536
      output_dim: 2
      features: 128
      out_channels: [96, 192, 384, 768]
  img_size: [518, 518]
  num_classes: 133
  enable_panoptic_eval: true
  mask_thresh: 0.8
  overlap_thresh: 0.8
  attn_mask_annealing_enabled: false
  num_masked_layers: 4
  lr:          0.0004      # ä» 2e-4 æåˆ° 4e-4
  llrd:        0.8         # ä¿æŒä¸å˜
  weight_decay: 0.04       # ä¿æŒä¸å˜
  warmup_steps: [500, 1000]
  poly_power:  0.9         # ä¿æŒä¸å˜

data:
  root: /home/jovyan/ybai_ws/dataset/dataset/coco
  panoptic_json_train: /home/jovyan/ybai_ws/dataset/dataset/coco/annotations/panoptic_train2017.json
  panoptic_json_val: /home/jovyan/ybai_ws/dataset/dataset/coco/annotations/panoptic_val2017.json
  batch_size_per_gpu: 10
  num_workers: 8
  img_size: [518, 518]
  stuff_classes: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]
